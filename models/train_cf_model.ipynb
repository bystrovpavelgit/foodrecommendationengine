{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90fa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6f284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Metrics(Callback):\n",
    "    \"\"\" F1Metrics\"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "        val_true = self.model.validation_data[1]\n",
    "        val_f1 = f1_score(val_true, val_predict, average='macro')\n",
    "        self.val_f1s.append(val_f1)\n",
    "        print(f\"F1={val_f1}\")\n",
    "        return\n",
    "\n",
    "\n",
    "metr = F1Metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa21b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ratings_file)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df[\"userId\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "movie_ids = df[\"movieId\"].unique().tolist()\n",
    "movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n",
    "movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n",
    "df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n",
    "df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n",
    "\n",
    "num_users = len(user2user_encoded)\n",
    "num_movies = len(movie_encoded2movie)\n",
    "df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n",
    "# min and max ratings will be used to normalize the ratings later\n",
    "min_rating = min(df[\"rating\"])\n",
    "max_rating = max(df[\"rating\"])\n",
    "\n",
    "print(\n",
    "    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n",
    "        num_users, num_movies, min_rating, max_rating\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96604406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42)\n",
    "x = df[[\"user\", \"movie\"]].values\n",
    "# Normalize the targets between 0 and 1. Makes it easy to train.\n",
    "y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n",
    "# Assuming training on 90% of the data and validating on 10%.\n",
    "train_indices = int(0.9 * df.shape[0])\n",
    "x_train, x_val, y_train, y_val = (\n",
    "    x[:train_indices],\n",
    "    x[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 60\n",
    "\n",
    "\n",
    "class RecommenderNet(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        # The sigmoid activation forces the rating to between 0 and 1\n",
    "        return tf.nn.sigmoid(x)\n",
    "\n",
    "\n",
    "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f20ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1108b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n",
    "\n",
    "# Let us get a user and see the top recommendations.\n",
    "user_id = df.userId.sample(1).iloc[0]\n",
    "movies_watched_by_user = df[df.userId == user_id]\n",
    "movies_not_watched = movie_df[\n",
    "    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n",
    "][\"movieId\"]\n",
    "movies_not_watched = list(\n",
    "    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n",
    ")\n",
    "movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n",
    "user_encoder = user2user_encoded.get(user_id)\n",
    "user_movie_array = np.hstack(\n",
    "    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n",
    ")\n",
    "ratings = model.predict(user_movie_array).flatten()\n",
    "top_ratings_indices = ratings.argsort()[-10:][::-1]\n",
    "recommended_movie_ids = [\n",
    "    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n",
    "]\n",
    "\n",
    "print(\"Showing recommendations for user: {}\".format(user_id))\n",
    "print(\"====\" * 9)\n",
    "print(\"Movies with high ratings from user\")\n",
    "print(\"----\" * 8)\n",
    "top_movies_user = (\n",
    "    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n",
    "    .head(5)\n",
    "    .movieId.values\n",
    ")\n",
    "movie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\n",
    "for row in movie_df_rows.itertuples():\n",
    "    print(row.title, \":\", row.genres)\n",
    "\n",
    "print(\"----\" * 8)\n",
    "print(\"Top 10 movie recommendations\")\n",
    "print(\"----\" * 8)\n",
    "recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\n",
    "for row in recommended_movies.itertuples():\n",
    "    print(row.title, \":\", row.genres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score (y_true, y_score, k = 20, gains = \"exponential\"):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score: array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k: int\n",
    "        Rank.\n",
    "    gains: str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    DCG@k: float\n",
    "    \"\"\"\n",
    "    order = np.argsort (y_score) [::-1]\n",
    "    y_true = np.take (y_true, order [: k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true-1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError (\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2 (np.arange (len (y_true)) + 2)\n",
    "    return np.sum (gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score (y_true, y_score, k = 20, gains = \"exponential\"):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank k\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score: array-like, shape = [n_samples]\n",
    "        Predicted scores.\n",
    "    k: int\n",
    "        Rank.\n",
    "    gains: str\n",
    "        Whether gains should be \"exponential\" (default) or \"linear\".\n",
    "    Returns\n",
    "    -------\n",
    "    NDCG @k: float\n",
    "    \"\"\"\n",
    "    best = dcg_score (y_true, y_true, k, gains)\n",
    "    actual = dcg_score (y_true, y_score, k, gains)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "\n",
    "# The fundamental definition of DCG is that it is a measure of ranking quality. This assumes that you have computed the utilities of each document/item and ranked them in a certain order.\n",
    "\n",
    "# With this definition in mind, if you have n-items with same utility (which is 0 in your case), computing NDCG to measure the ranking quality within this subset of items (since you are only looking at items 5, 4, 3, 2 and 1, all of which are not recommended), will yield you a NDCG score of 1 - since your ranking is perfect if you are only looking at these items.\n",
    "\n",
    "# NDCG is merely a way to quantify the quality of ordering, i.e., current order Vs perfect order (items sorted w.r.to their utilities). This is meaningless if you are looking ONLY at items with same utility score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4b4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
