{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ba3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "18114 8979\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "DIM = 5\n",
    "\n",
    "\n",
    "def put_zeros(arr):\n",
    "    for i in range(len(arr) - 1, -1, -1):\n",
    "        if arr[i] != 1:\n",
    "            break\n",
    "        arr[i] = 0\n",
    "    return arr\n",
    "\n",
    "\n",
    "def fill_with_zeros(texts):\n",
    "    result = [[put_zeros(x[0]), put_zeros(x[1])] for x in texts]\n",
    "    return result\n",
    "\n",
    "\n",
    "def concat_arr(texts):\n",
    "    result = np.array([np.array(x[0] + x[1][:100]) for x in texts])\n",
    "    return result\n",
    "\n",
    "\n",
    "with open(\"../num_texts.pkl\", \"rb\") as f:\n",
    "    num_texts = pickle.load(f)\n",
    "with open(\"vocabulary.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open(\"../y_gt2.pkl\", \"rb\") as f:\n",
    "    y_true = pickle.load(f)\n",
    "    \n",
    "y_gt = np.zeros(shape=(len(y_true), DIM))\n",
    "for i in range(len(y_true)):\n",
    "    y_gt[i][y_true[i]-1]=1\n",
    "\n",
    "max_words = len(vocab)\n",
    "result = concat_arr(fill_with_zeros(num_texts))\n",
    "ndx = list(range(len(y_true)))\n",
    "np.random.shuffle(ndx)\n",
    "\n",
    "train_feats, test_feats = result[ndx[:15080]], result[ndx[15080:]]\n",
    "train_lbl, test_lbl = y_gt[ndx[:15080]], y_gt[ndx[15080:]]\n",
    "print(len(y_true), max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912c0d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 18:23:49.618523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-14 18:23:49.618787: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-14 18:23:49.618825: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PDC003): /proc/driver/nvidia/version does not exist\n",
      "2022-10-14 18:23:49.619313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          898000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 500, 56)          28896     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 56)               19040     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 285       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 946,221\n",
      "Trainable params: 946,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/24\n",
      "59/59 [==============================] - 234s 4s/step - loss: 1.0483 - acc: 0.5891 - val_loss: 0.5388 - val_acc: 0.8266 - lr: 0.0023\n",
      "Epoch 2/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.3145 - acc: 0.9006 - val_loss: 0.1561 - val_acc: 0.9585 - lr: 0.0022\n",
      "Epoch 3/24\n",
      "59/59 [==============================] - 228s 4s/step - loss: 0.0935 - acc: 0.9768 - val_loss: 0.0635 - val_acc: 0.9852 - lr: 0.0021\n",
      "Epoch 4/24\n",
      "59/59 [==============================] - 228s 4s/step - loss: 0.0373 - acc: 0.9912 - val_loss: 0.0276 - val_acc: 0.9937 - lr: 0.0020\n",
      "Epoch 5/24\n",
      "59/59 [==============================] - 226s 4s/step - loss: 0.0198 - acc: 0.9952 - val_loss: 0.0123 - val_acc: 0.9957 - lr: 0.0018\n",
      "Epoch 6/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0143 - val_acc: 0.9964 - lr: 0.0017\n",
      "Epoch 7/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0086 - acc: 0.9981 - val_loss: 0.0125 - val_acc: 0.9974 - lr: 0.0016\n",
      "Epoch 8/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0043 - val_acc: 0.9987 - lr: 0.0015\n",
      "Epoch 9/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0045 - acc: 0.9990 - val_loss: 0.0021 - val_acc: 1.0000 - lr: 0.0014\n",
      "Epoch 10/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0023 - acc: 0.9997 - val_loss: 0.0013 - val_acc: 1.0000 - lr: 0.0013\n",
      "Epoch 11/24\n",
      "59/59 [==============================] - 219s 4s/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.0047 - val_acc: 0.9993 - lr: 0.0013\n",
      "Epoch 12/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0030 - acc: 0.9995 - val_loss: 7.2529e-04 - val_acc: 1.0000 - lr: 0.0012\n",
      "Epoch 13/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0011 - acc: 0.9999 - val_loss: 7.2459e-04 - val_acc: 1.0000 - lr: 0.0011\n",
      "Epoch 14/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 8.5716e-04 - acc: 1.0000 - val_loss: 5.0714e-04 - val_acc: 1.0000 - lr: 0.0011\n",
      "Epoch 15/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0048 - acc: 0.9989 - val_loss: 4.9239e-04 - val_acc: 1.0000 - lr: 9.8823e-04\n",
      "Epoch 16/24\n",
      "59/59 [==============================] - 217s 4s/step - loss: 0.0025 - acc: 0.9996 - val_loss: 6.3100e-04 - val_acc: 1.0000 - lr: 9.2894e-04\n",
      "Epoch 17/24\n",
      "59/59 [==============================] - 217s 4s/step - loss: 0.0013 - acc: 0.9998 - val_loss: 4.3449e-04 - val_acc: 1.0000 - lr: 8.7320e-04\n",
      "Epoch 18/24\n",
      "59/59 [==============================] - 226s 4s/step - loss: 7.4963e-04 - acc: 0.9999 - val_loss: 4.7390e-04 - val_acc: 1.0000 - lr: 8.2081e-04\n",
      "Epoch 19/24\n",
      "59/59 [==============================] - 228s 4s/step - loss: 9.7903e-04 - acc: 0.9998 - val_loss: 3.6252e-04 - val_acc: 1.0000 - lr: 7.7156e-04\n",
      "Epoch 20/24\n",
      "59/59 [==============================] - 227s 4s/step - loss: 9.5125e-04 - acc: 0.9997 - val_loss: 3.4570e-04 - val_acc: 1.0000 - lr: 7.2527e-04\n",
      "Epoch 21/24\n",
      "59/59 [==============================] - 227s 4s/step - loss: 7.8839e-04 - acc: 0.9999 - val_loss: 3.0586e-04 - val_acc: 1.0000 - lr: 6.8175e-04\n",
      "Epoch 22/24\n",
      "59/59 [==============================] - 227s 4s/step - loss: 5.3818e-04 - acc: 0.9999 - val_loss: 2.8015e-04 - val_acc: 1.0000 - lr: 6.4084e-04\n",
      "Epoch 23/24\n",
      "59/59 [==============================] - 227s 4s/step - loss: 4.0310e-04 - acc: 1.0000 - val_loss: 2.7172e-04 - val_acc: 1.0000 - lr: 6.0239e-04\n",
      "Epoch 24/24\n",
      "59/59 [==============================] - 229s 4s/step - loss: 0.0012 - acc: 0.9998 - val_loss: 2.7073e-04 - val_acc: 1.0000 - lr: 5.6625e-04\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "## train cuisine type classifier using Bidirectional RNN\n",
    "####################################################\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def schedule(ep, lr):\n",
    "    return lr * 0.95\n",
    "\n",
    "\n",
    "seed = 111\n",
    "np.random.seed(seed)\n",
    "learning_rate = 0.0025 # initial learning rate\n",
    "batch_size = 256 # Batch size for training\n",
    "mod = keras.models.Sequential()\n",
    "mod.add(layers.Embedding(max_words+1, 100, input_length=500))\n",
    "mod.add(Bidirectional(LSTM(28, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)))\n",
    "mod.add(Bidirectional(LSTM(28, dropout=0.3, recurrent_dropout=0.2)))\n",
    "mod.add(Dense(DIM, activation=\"softmax\"))\n",
    "mod.summary()\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = Adam(learning_rate=learning_rate)\n",
    "sch = LearningRateScheduler(schedule)\n",
    "sch2 = callbacks.ModelCheckpoint(filepath=\"./rnn2_with_embed.h5\", monitor=\"val_acc\", save_best_only=True)\n",
    "mod.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "hist = mod.fit(train_feats, train_lbl, epochs=24, batch_size=batch_size, callbacks=[sch, sch2],\n",
    "               validation_data=(test_feats, test_lbl), shuffle=True)\n",
    "mod.save(\"./2way_rnn2_last.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388ba013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567/567 [==============================] - 90s 159ms/step - loss: 2.5649e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.000256494851782918, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = mod.evaluate(x=result, y=y_gt)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fdd0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18114 (15080, 500) (3034, 7)\n"
     ]
    }
   ],
   "source": [
    "DIM=7\n",
    "with open(\"../y_gt1.pkl\", \"rb\") as f:\n",
    "    y_true = pickle.load(f)\n",
    "    \n",
    "y_gt = np.zeros(shape=(len(y_true), DIM))\n",
    "for i in range(len(y_true)):\n",
    "    y_gt[i][y_true[i]-1]=1\n",
    "\n",
    "max_words = len(vocab)\n",
    "result = concat_arr(fill_with_zeros(num_texts))\n",
    "ndx = list(range(len(y_true)))\n",
    "np.random.shuffle(ndx)\n",
    "\n",
    "train_feats, test_feats = result[ndx[:15080]], result[ndx[15080:]]\n",
    "train_lbl, test_lbl = y_gt[ndx[:15080]], y_gt[ndx[15080:]]\n",
    "print(len(y_true),train_feats.shape, test_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab99266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 20:20:26.076631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-14 20:20:26.077024: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-14 20:20:26.077065: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PDC003): /proc/driver/nvidia/version does not exist\n",
      "2022-10-14 20:20:26.077924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          898000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 500, 56)          28896     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 56)               19040     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 399       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 946,335\n",
      "Trainable params: 946,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 1.2494 - acc: 0.5634 - val_loss: 0.4213 - val_acc: 0.8817 - lr: 0.0023\n",
      "Epoch 2/24\n",
      "59/59 [==============================] - 212s 4s/step - loss: 0.2378 - acc: 0.9370 - val_loss: 0.1115 - val_acc: 0.9726 - lr: 0.0022\n",
      "Epoch 3/24\n",
      "59/59 [==============================] - 218s 4s/step - loss: 0.0712 - acc: 0.9855 - val_loss: 0.0376 - val_acc: 0.9927 - lr: 0.0021\n",
      "Epoch 4/24\n",
      "59/59 [==============================] - 221s 4s/step - loss: 0.0415 - acc: 0.9916 - val_loss: 0.0590 - val_acc: 0.9825 - lr: 0.0020\n",
      "Epoch 5/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0266 - acc: 0.9944 - val_loss: 0.0242 - val_acc: 0.9941 - lr: 0.0018\n",
      "Epoch 6/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0167 - acc: 0.9969 - val_loss: 0.0230 - val_acc: 0.9954 - lr: 0.0017\n",
      "Epoch 7/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0132 - acc: 0.9977 - val_loss: 0.0208 - val_acc: 0.9960 - lr: 0.0016\n",
      "Epoch 8/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0100 - acc: 0.9984 - val_loss: 0.0129 - val_acc: 0.9984 - lr: 0.0015\n",
      "Epoch 9/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0078 - acc: 0.9987 - val_loss: 0.0077 - val_acc: 0.9984 - lr: 0.0014\n",
      "Epoch 10/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0076 - acc: 0.9985 - val_loss: 0.0090 - val_acc: 0.9984 - lr: 0.0013\n",
      "Epoch 11/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0122 - acc: 0.9967 - val_loss: 0.0062 - val_acc: 0.9987 - lr: 0.0013\n",
      "Epoch 12/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.0036 - val_acc: 0.9987 - lr: 0.0012\n",
      "Epoch 13/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0031 - acc: 0.9995 - val_loss: 0.0031 - val_acc: 0.9987 - lr: 0.0011\n",
      "Epoch 14/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0027 - val_acc: 0.9987 - lr: 0.0011\n",
      "Epoch 15/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0091 - acc: 0.9978 - val_loss: 0.0029 - val_acc: 0.9993 - lr: 9.8823e-04\n",
      "Epoch 16/24\n",
      "59/59 [==============================] - 222s 4s/step - loss: 0.0030 - acc: 0.9995 - val_loss: 0.0045 - val_acc: 0.9980 - lr: 9.2894e-04\n",
      "Epoch 17/24\n",
      "59/59 [==============================] - 222s 4s/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0015 - val_acc: 1.0000 - lr: 8.7320e-04\n",
      "Epoch 18/24\n",
      "59/59 [==============================] - 222s 4s/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0013 - val_acc: 1.0000 - lr: 8.2081e-04\n",
      "Epoch 19/24\n",
      "59/59 [==============================] - 223s 4s/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.0011 - val_acc: 1.0000 - lr: 7.7156e-04\n",
      "Epoch 20/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0016 - acc: 0.9997 - val_loss: 9.3331e-04 - val_acc: 1.0000 - lr: 7.2527e-04\n",
      "Epoch 21/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0011 - acc: 1.0000 - val_loss: 8.1097e-04 - val_acc: 1.0000 - lr: 6.8175e-04\n",
      "Epoch 22/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0014 - acc: 0.9999 - val_loss: 9.7853e-04 - val_acc: 1.0000 - lr: 6.4084e-04\n",
      "Epoch 23/24\n",
      "59/59 [==============================] - 224s 4s/step - loss: 0.0015 - acc: 0.9999 - val_loss: 7.9839e-04 - val_acc: 1.0000 - lr: 6.0239e-04\n",
      "Epoch 24/24\n",
      "59/59 [==============================] - 226s 4s/step - loss: 9.2038e-04 - acc: 0.9999 - val_loss: 6.7652e-04 - val_acc: 1.0000 - lr: 5.6625e-04\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "## train cuisine type classifier using Bidirectional RNN\n",
    "####################################################\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def schedule(ep, lr):\n",
    "    return lr * 0.93\n",
    "\n",
    "\n",
    "seed = 111\n",
    "np.random.seed(seed)\n",
    "learning_rate = 0.0025 # initial learning rate\n",
    "batch_size = 256 # Batch size for training\n",
    "mod = keras.models.Sequential()\n",
    "mod.add(layers.Embedding(max_words+1, 100, input_length=500))\n",
    "mod.add(Bidirectional(LSTM(28, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)))\n",
    "mod.add(Bidirectional(LSTM(28, dropout=0.3, recurrent_dropout=0.2)))\n",
    "mod.add(Dense(DIM, activation=\"softmax\"))\n",
    "mod.summary()\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = Adam(learning_rate=learning_rate)\n",
    "sch = LearningRateScheduler(schedule)\n",
    "sch2 = callbacks.ModelCheckpoint(filepath=\"./rnn2_dish_pred7.h5\", monitor=\"val_acc\", save_best_only=True)\n",
    "mod.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "hist = mod.fit(train_feats, train_lbl, epochs=24, batch_size=batch_size, callbacks=[sch, sch2],\n",
    "               validation_data=(test_feats, test_lbl), shuffle=True)\n",
    "mod.save(\"./rnn2_dish_pred7_last.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
